{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "VGG16+Res10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMyxl59RBhOSuBqvtSdLiOz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Soedirman-Machine-Learning/Emotion-detection/blob/main/VGG16%2BRes10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5pzawlYeCdv",
        "outputId": "2ad9d130-8481-4957-aaec-0a670b7790f2"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmVz4psfepiJ"
      },
      "source": [
        "#Menyimpan hasil ekstrak ke dalam folder datasets\r\n",
        "import zipfile,os\r\n",
        "local_zip = \"/content/drive/MyDrive/Colab Notebooks/Dataset_Emosi.zip\"\r\n",
        "zip_ref = zipfile.ZipFile(local_zip, \"r\")\r\n",
        "zip_ref.extractall(\"/content/datasets\")\r\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvL-AVpNesbU",
        "outputId": "893ed457-8a39-40f5-eb4b-2adcf5002447"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "from tensorflow.keras.applications import MobileNet\r\n",
        "from tensorflow.keras.layers import AveragePooling2D\r\n",
        "from tensorflow.keras.layers import Dropout\r\n",
        "from tensorflow.keras.layers import Flatten\r\n",
        "from tensorflow.keras.layers import Dense\r\n",
        "from tensorflow.keras.layers import Input\r\n",
        "from tensorflow.keras.models import Model\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\r\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\r\n",
        "from tensorflow.keras.preprocessing.image import load_img\r\n",
        "from tensorflow.keras.utils import to_categorical\r\n",
        "from sklearn.preprocessing import LabelBinarizer\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from imutils import paths\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import argparse\r\n",
        "import os\r\n",
        "from skimage.io import imread\r\n",
        "from skimage.transform import resize\r\n",
        "import cv2\r\n",
        "from keras.models import Sequential\r\n",
        "\r\n",
        " \r\n",
        "# Mengihitung waktu lamanya eksekusi tiap sel di Google Colab\r\n",
        "!pip install ipython-autotime\r\n",
        " \r\n",
        "%load_ext autotime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ipython-autotime\n",
            "  Downloading https://files.pythonhosted.org/packages/b4/c9/b413a24f759641bc27ef98c144b590023c8038dfb8a3f09e713e9dff12c1/ipython_autotime-0.3.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from ipython-autotime) (5.5.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (2.6.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (4.3.3)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (0.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (51.3.3)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (1.0.18)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->ipython-autotime) (0.7.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->ipython-autotime) (0.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->ipython-autotime) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (0.2.5)\n",
            "Installing collected packages: ipython-autotime\n",
            "Successfully installed ipython-autotime-0.3.1\n",
            "time: 137 Âµs (started: 2021-01-24 16:56:22 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzavYyZ-exCD",
        "outputId": "2dbe3eee-1d63-4855-e90f-d6638528d654"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.0\n",
            "time: 2.72 ms (started: 2021-01-24 16:56:22 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wa-Q30Rbe0dK",
        "outputId": "63d5cc55-fb20-4162-88e1-7d576aea9c55"
      },
      "source": [
        "datagen = ImageDataGenerator(\r\n",
        "    rescale=1./255, \r\n",
        "    validation_split = 0.2,\r\n",
        "    rotation_range = 20,\r\n",
        "    horizontal_flip = True,\r\n",
        "    shear_range = 0.2,\r\n",
        "    zoom_range = 0.2,    \r\n",
        "    vertical_flip = True,\r\n",
        "    fill_mode = \"nearest\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1.57 ms (started: 2021-01-24 16:56:22 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQ4R8HA9e34b",
        "outputId": "563ec126-7df6-4187-d12c-ca2e8beb85d4"
      },
      "source": [
        "#Memuat dataset pelatihan & validasi\r\n",
        "base_dir = os.path.join(\"/content/datasets/Dataset_Emosi/Train\")\r\n",
        " \r\n",
        "img_size =224\r\n",
        "batch_size = 128\r\n",
        " \r\n",
        "train_generator = datagen.flow_from_directory(\r\n",
        "    base_dir,\r\n",
        "    target_size=(img_size, img_size),\r\n",
        "    batch_size=batch_size, \r\n",
        "    subset='training')\r\n",
        " \r\n",
        "val_generator = datagen.flow_from_directory(\r\n",
        "    base_dir,\r\n",
        "    target_size=(img_size, img_size),\r\n",
        "    batch_size=batch_size, \r\n",
        "    subset='validation')\r\n",
        "\r\n",
        " \r\n",
        "#Memuat dataset pengujian\r\n",
        "X_test = []\r\n",
        "y_test = []\r\n",
        "labels = ['Marah',\r\n",
        "          'Sedih',\r\n",
        "          'Senang']\r\n",
        " \r\n",
        "for i,label in enumerate(labels):\r\n",
        "    folder = os.path.join(\"/content/datasets/Dataset_Emosi/Test\",label)\r\n",
        "    files = sorted(os.listdir(folder))\r\n",
        "    files = [x for x in files if x.endswith(\".jpg\")]\r\n",
        "    for k,file in enumerate(files):\r\n",
        "        image_path = os.path.join(folder, file)       \r\n",
        "        image = imread(image_path)/255.\r\n",
        "        image = resize(image,(224,224))\r\n",
        "        X_test.append(image)\r\n",
        "        category = os.path.split(folder)[-1]\r\n",
        "        y_test.append(i)\r\n",
        " \r\n",
        "X_test = np.array(X_test)\r\n",
        "y_test = np.array(y_test)\r\n",
        "\r\n",
        "\r\n",
        " \r\n",
        "#Menampilkan bentuk dari masing-masing dataset\r\n",
        "for image_batch, label_batch in train_generator:\r\n",
        "  break\r\n",
        "print(\"Bentuk array dari dataset train (pelatihan) adalah:\", image_batch.shape,label_batch.shape)\r\n",
        "for image_batch, label_batch in val_generator:\r\n",
        "  break\r\n",
        "print(\"Bentuk array dari dataset validation (validasi) adalah:\", image_batch.shape,label_batch.shape)\r\n",
        "print(\"Bentuk array dari dataset test (pengujian) adalah:\", X_test.shape,y_test.shape,)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 654 images belonging to 3 classes.\n",
            "Found 162 images belonging to 3 classes.\n",
            "Bentuk array dari dataset train (pelatihan) adalah: (128, 224, 224, 3) (128, 3)\n",
            "Bentuk array dari dataset validation (validasi) adalah: (128, 224, 224, 3) (128, 3)\n",
            "Bentuk array dari dataset test (pengujian) adalah: (204, 224, 224, 3) (204,)\n",
            "time: 7 s (started: 2021-01-24 16:56:22 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJ4F-Daoe9Te",
        "outputId": "2b79ee7c-880f-4675-ddec-5f950517a01d"
      },
      "source": [
        "print (train_generator.class_indices)\r\n",
        "\r\n",
        "labels_txt = '\\n'.join(sorted(train_generator.class_indices.keys()))\r\n",
        "\r\n",
        "with open('labels.txt', 'w') as f:\r\n",
        "  f.write(labels_txt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Marah': 0, 'Sedih': 1, 'Senang': 2}\n",
            "time: 3.9 ms (started: 2021-01-24 16:56:29 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exp4_6BkfAoF",
        "outputId": "ea155a72-53c0-4756-e946-3f661dd6c410"
      },
      "source": [
        "IMG_SHAPE = (224,224, 3)\r\n",
        "# Membuat model dasar (base model) dari pre-trained model \r\n",
        "base_model = tf.keras.applications.VGG16(input_shape=IMG_SHAPE,\r\n",
        "                                          include_top=False,\r\n",
        "                                          weights='imagenet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "time: 6.16 s (started: 2021-01-24 16:56:29 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRQZUQlufDni",
        "outputId": "ff7c9947-3037-4843-8e9e-5b42b43331d3"
      },
      "source": [
        "base_model.trainable = False\r\n",
        "base_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 0\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "time: 10.5 ms (started: 2021-01-24 16:56:35 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xdXVI-PfHMT",
        "outputId": "46cc4110-a035-4702-b4ee-b6d35d2d9ff7"
      },
      "source": [
        "model = tf.keras.Sequential([\r\n",
        "                             base_model,\r\n",
        "                             tf.keras.layers.Flatten(),\r\n",
        "                             tf.keras.layers.Dense(512, activation='relu'),\r\n",
        "                             tf.keras.layers.Dropout(rate=0.25),\r\n",
        "                             tf.keras.layers.Dense(3, activation='softmax')\r\n",
        "])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 69.6 ms (started: 2021-01-24 16:56:35 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cd8SLVfSfKNB",
        "outputId": "d2d53a11-167d-4503-e3cd-6bea4e3a6d16"
      },
      "source": [
        "model.compile(\"adam\",loss=\"categorical_crossentropy\",metrics=[\"acc\"])\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vgg16 (Functional)           (None, 7, 7, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               12845568  \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 1539      \n",
            "=================================================================\n",
            "Total params: 27,561,795\n",
            "Trainable params: 12,847,107\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "time: 23.3 ms (started: 2021-01-24 16:56:35 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KPT8KiqfMKu",
        "outputId": "a58dcfc6-c422-4c48-f29b-7fbf88e046cf"
      },
      "source": [
        "#es = EarlyStopping(monitor=\"val_loss\", patience=7, verbose=1, min_delta=0.09, mode=\"auto\")\r\n",
        "history = model.fit(train_generator,\r\n",
        "                    epochs=50,\r\n",
        "                    validation_data=val_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "6/6 [==============================] - 22s 2s/step - loss: 9.2136 - acc: 0.3510 - val_loss: 1.7075 - val_acc: 0.5309\n",
            "Epoch 2/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.8586 - acc: 0.4468 - val_loss: 1.5583 - val_acc: 0.4259\n",
            "Epoch 3/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.4386 - acc: 0.5099 - val_loss: 1.0773 - val_acc: 0.5494\n",
            "Epoch 4/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 1.0228 - acc: 0.5990 - val_loss: 0.6961 - val_acc: 0.6975\n",
            "Epoch 5/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.8394 - acc: 0.6511 - val_loss: 1.0053 - val_acc: 0.5988\n",
            "Epoch 6/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.9442 - acc: 0.6069 - val_loss: 0.8468 - val_acc: 0.5679\n",
            "Epoch 7/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.7868 - acc: 0.6257 - val_loss: 0.8808 - val_acc: 0.5926\n",
            "Epoch 8/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.7038 - acc: 0.7052 - val_loss: 0.6643 - val_acc: 0.7346\n",
            "Epoch 9/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.5920 - acc: 0.7543 - val_loss: 0.6839 - val_acc: 0.6914\n",
            "Epoch 10/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.5912 - acc: 0.7445 - val_loss: 0.7105 - val_acc: 0.6914\n",
            "Epoch 11/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.6100 - acc: 0.7447 - val_loss: 0.6964 - val_acc: 0.6667\n",
            "Epoch 12/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.5641 - acc: 0.7610 - val_loss: 0.6518 - val_acc: 0.7654\n",
            "Epoch 13/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.5525 - acc: 0.7694 - val_loss: 0.6235 - val_acc: 0.7407\n",
            "Epoch 14/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.4899 - acc: 0.7977 - val_loss: 0.6627 - val_acc: 0.6914\n",
            "Epoch 15/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.5223 - acc: 0.7636 - val_loss: 0.5607 - val_acc: 0.7531\n",
            "Epoch 16/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.4236 - acc: 0.8374 - val_loss: 0.6266 - val_acc: 0.7346\n",
            "Epoch 17/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.4350 - acc: 0.8421 - val_loss: 0.6090 - val_acc: 0.7407\n",
            "Epoch 18/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.3857 - acc: 0.8484 - val_loss: 0.6209 - val_acc: 0.7346\n",
            "Epoch 19/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.3415 - acc: 0.8834 - val_loss: 0.5344 - val_acc: 0.7593\n",
            "Epoch 20/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.4021 - acc: 0.8449 - val_loss: 0.6365 - val_acc: 0.7407\n",
            "Epoch 21/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.3400 - acc: 0.8803 - val_loss: 0.6469 - val_acc: 0.7222\n",
            "Epoch 22/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.3597 - acc: 0.8770 - val_loss: 0.5674 - val_acc: 0.7593\n",
            "Epoch 23/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.3807 - acc: 0.8359 - val_loss: 0.7016 - val_acc: 0.7284\n",
            "Epoch 24/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.3708 - acc: 0.8507 - val_loss: 0.5969 - val_acc: 0.7284\n",
            "Epoch 25/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.3817 - acc: 0.8587 - val_loss: 0.5240 - val_acc: 0.7778\n",
            "Epoch 26/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.4159 - acc: 0.8334 - val_loss: 0.6609 - val_acc: 0.7346\n",
            "Epoch 27/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.3585 - acc: 0.8681 - val_loss: 0.5751 - val_acc: 0.7407\n",
            "Epoch 28/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.3331 - acc: 0.8666 - val_loss: 0.5888 - val_acc: 0.7407\n",
            "Epoch 29/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.3658 - acc: 0.8686 - val_loss: 0.8255 - val_acc: 0.6728\n",
            "Epoch 30/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.3224 - acc: 0.8918 - val_loss: 0.4818 - val_acc: 0.8210\n",
            "Epoch 31/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.3322 - acc: 0.8667 - val_loss: 0.6305 - val_acc: 0.7407\n",
            "Epoch 32/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.3406 - acc: 0.8757 - val_loss: 0.5563 - val_acc: 0.7778\n",
            "Epoch 33/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.2990 - acc: 0.8848 - val_loss: 0.4957 - val_acc: 0.8210\n",
            "Epoch 34/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.2835 - acc: 0.8884 - val_loss: 0.5660 - val_acc: 0.7531\n",
            "Epoch 35/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.3477 - acc: 0.8737 - val_loss: 0.6411 - val_acc: 0.7284\n",
            "Epoch 36/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.3088 - acc: 0.8761 - val_loss: 0.5157 - val_acc: 0.7840\n",
            "Epoch 37/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.3342 - acc: 0.8734 - val_loss: 0.6319 - val_acc: 0.7222\n",
            "Epoch 38/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.3822 - acc: 0.8650 - val_loss: 0.6044 - val_acc: 0.7531\n",
            "Epoch 39/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.2904 - acc: 0.8803 - val_loss: 0.4852 - val_acc: 0.7901\n",
            "Epoch 40/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.2571 - acc: 0.9069 - val_loss: 0.6695 - val_acc: 0.7469\n",
            "Epoch 41/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.2612 - acc: 0.9048 - val_loss: 0.5957 - val_acc: 0.7840\n",
            "Epoch 42/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.2196 - acc: 0.9352 - val_loss: 0.6418 - val_acc: 0.7284\n",
            "Epoch 43/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.3021 - acc: 0.8941 - val_loss: 0.6036 - val_acc: 0.7716\n",
            "Epoch 44/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.2263 - acc: 0.9238 - val_loss: 0.5374 - val_acc: 0.7778\n",
            "Epoch 45/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.2657 - acc: 0.8956 - val_loss: 0.5414 - val_acc: 0.8086\n",
            "Epoch 46/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.2199 - acc: 0.9249 - val_loss: 0.5200 - val_acc: 0.7840\n",
            "Epoch 47/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.2332 - acc: 0.9153 - val_loss: 0.5485 - val_acc: 0.7778\n",
            "Epoch 48/50\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.2495 - acc: 0.9075 - val_loss: 0.5480 - val_acc: 0.8025\n",
            "Epoch 49/50\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2375 - acc: 0.9093"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Urgsw2NdfPiM"
      },
      "source": [
        "fig = plt.figure(figsize=(7, 4))\r\n",
        "fig.set_figheight(10)\r\n",
        "fig.set_figwidth(15)\r\n",
        " \r\n",
        "plt.subplot(2, 2, 1)\r\n",
        "plt.plot(history.history[\"acc\"], label = \"Training Accuracy\")\r\n",
        "plt.plot(history.history[\"val_acc\"], label=\"Validation Accuracy\")\r\n",
        "plt.legend()\r\n",
        "plt.title(\"Kurva Tingkat Akurasi\", size=15)\r\n",
        "plt.grid(zorder=0)\r\n",
        " \r\n",
        "plt.subplot(2, 2, 2)\r\n",
        "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\r\n",
        "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\r\n",
        "plt.legend()\r\n",
        "plt.title(\"Kurva Tingkat Error\", size=15)\r\n",
        "plt.grid(zorder=0)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOS0s2pxfUBY"
      },
      "source": [
        "print(train_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKFlLGoVfVq4"
      },
      "source": [
        "y_test2 = to_categorical(y_test)\r\n",
        "X_test3, y_test3 = (X_test, y_test2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y00ee2o3fX7Z"
      },
      "source": [
        "#Menampilkan matriks yang benar dan matriks hasil prediksi\r\n",
        "\r\n",
        "#Label yang benar\r\n",
        "y_true = np.argmax(y_test2,axis=1)\r\n",
        "\r\n",
        "#Label prediksi\r\n",
        "Y_pred = model.predict(X_test)\r\n",
        "y_pred = np.argmax(Y_pred, axis=1)\r\n",
        "\r\n",
        "print(y_true)\r\n",
        "print(y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "artyh2acfbBJ"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\r\n",
        "from sklearn.utils.multiclass import unique_labels\r\n",
        "\r\n",
        "def plot_confusion_matrix(y_true, y_pred, classes,\r\n",
        "                          normalize=False,\r\n",
        "                          title=None,\r\n",
        "                          cmap=plt.cm.Blues):\r\n",
        "  \"\"\"\r\n",
        "  This function prints and plots the confusion matrix.\r\n",
        "  Normalization can be applied by setting `normalize=True`.\r\n",
        "  \"\"\"\r\n",
        "  if not title:\r\n",
        "    if normalize:\r\n",
        "      title = 'Normalized confusion matrix'\r\n",
        "    else:\r\n",
        "      title = 'Confusion matrix, without normalization'\r\n",
        "\r\n",
        "  #compute confusion matrix\r\n",
        "  cm = confusion_matrix(y_true, y_pred)\r\n",
        "  #Only use the labels that appear in the data\r\n",
        "  #classes = classes[unique_labels(y_true, y_pred)]\r\n",
        "  if normalize:\r\n",
        "    cm = cm.astype('float') / cm.sum(axis=1)[:, np. newaxis]\r\n",
        "    print(\"Normalized confusion matrix\")\r\n",
        "  else:\r\n",
        "    print('Confusion matrix, without normalization')\r\n",
        "\r\n",
        "  print(cm)\r\n",
        "\r\n",
        "  fig, ax = plt.subplots(figsize=(5,5))\r\n",
        "  im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\r\n",
        "  #ax.figure.colorbar(im, ax=ax)\r\n",
        "  #we want to show all ticks...\r\n",
        "  ax.set(xticks=np.arange(cm.shape[1]),\r\n",
        "         yticks=np.arange(cm.shape[0]),\r\n",
        "         #...and label them with the respective list entries\r\n",
        "         xticklabels=classes, yticklabels=classes,\r\n",
        "         title=title,\r\n",
        "         ylabel='Label Benar',\r\n",
        "         xlabel='Label Prediksi')\r\n",
        "  \r\n",
        "  #Rotate the tick labels and set their alignment.\r\n",
        "  plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\r\n",
        "           rotation_mode=\"anchor\")\r\n",
        "  #Loop over data dimensions and create text annotations.\r\n",
        "  fmt = '.2f' if normalize else 'd'\r\n",
        "  thresh = cm.max() / 2.\r\n",
        "  for i in range(cm.shape[0]):\r\n",
        "    for j in range(cm.shape[1]):\r\n",
        "      ax.text(j, i, format(cm[i, j], fmt),\r\n",
        "              ha=\"center\", va=\"center\",\r\n",
        "              color=\"white\" if cm[i, j] > thresh else \"black\")\r\n",
        "      fig.tight_layout()\r\n",
        "      return ax\r\n",
        "\r\n",
        "np.set_printoptions(precision=2)\r\n",
        "\r\n",
        "plot_confusion_matrix(y_true, y_pred, classes=labels, normalize=True,\r\n",
        "                      title='Normalized confusion matrix')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75Z5i9tEfexq"
      },
      "source": [
        "saved_model_dir = 'save/model'\r\n",
        "tf.saved_model.save(model, saved_model_dir)\r\n",
        "\r\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\n",
        "tflite_model = converter.convert()\r\n",
        "\r\n",
        "with open('Emotion_Detection_MobileNet.tflite', 'wb') as f:\r\n",
        "  f.write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfkMA2P9fiwV"
      },
      "source": [
        "saved_model_dir = 'save/model'\r\n",
        "tf.saved_model.save(model, saved_model_dir)\r\n",
        "\r\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n",
        "tflite_quant_model = converter.convert()\r\n",
        "\r\n",
        "with open('Emotion_Detection_MobileNet_Quantized', 'wb') as f:\r\n",
        "  f.write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjBSEGuffnR6"
      },
      "source": [
        "n = 196\r\n",
        "\r\n",
        "plt.imshow(X_test[n])\r\n",
        "plt.show()\r\n",
        "\r\n",
        "true_label = np.argmax(y_test2,axis=1) [n]\r\n",
        "print(\"Label yang benar adalah:\", true_label,\":\", labels[true_label])\r\n",
        "prediction = model.predict(X_test[n][np.newaxis,...])[0]\r\n",
        "print(\"Nilai yang diprediksi adalah:\",prediction)\r\n",
        "predicted_label = np.argmax(prediction)\r\n",
        "print(\"Label yang diprediksi adalah:\",predicted_label,\":\",labels[predicted_label])\r\n",
        "\r\n",
        "if true_label == predicted_label:\r\n",
        "  print(\"Prediksi benar\")\r\n",
        "else:\r\n",
        "  print(\"Prediksi salah\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xS08-PjVfqmM"
      },
      "source": [
        "import cv2\r\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWz0j5VWfsHY"
      },
      "source": [
        "image = cv2.imread('/content/drive/MyDrive/Colab Notebooks/PENGUJIAN/senyum14.jpeg')\r\n",
        "orig = image.copy()\r\n",
        "(h,w) = image.shape[:2]\r\n",
        "\r\n",
        "blob = cv2.dnn.blobFromImage(image, 1.0, (300,300),\r\n",
        "                             (104.0, 177.0, 123.0))\r\n",
        "\r\n",
        "net = cv2.dnn.readNet('/content/drive/MyDrive/Colab Notebooks/Face_detector/deploy.prototxt', '/content/drive/MyDrive/Colab Notebooks/Face_detector/res10_300x300_ssd_iter_140000.caffemodel')\r\n",
        "\r\n",
        "#melewatkan blob melalui jaringan dan mendapatkan deteksi wajah\r\n",
        "print(\"Mendeteksi Wajah...\")\r\n",
        "net.setInput(blob)\r\n",
        "detections = net.forward()\r\n",
        "\r\n",
        "for i in range(0, detections.shape[0]):\r\n",
        "  #ekstrak keyakinan (probabilitas) yang terkait dengan deteksi\r\n",
        "  confidence = detections[0, 0, i, 2]\r\n",
        "  \r\n",
        "  if confidence > 0.5:\r\n",
        "    box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\r\n",
        "    (startX, startY, endX, endY) = box.astype(\"int\")\r\n",
        "\r\n",
        "    (startX, startY) = (max(0, startX), max(0, startY))\r\n",
        "    (endX, endY) = (min(w-1, endX), min(h-1, endY))\r\n",
        "  \r\n",
        "  # Ekstrak ROI wajah, konversikan dari BGR ke pemesanan saluran RGB,\r\n",
        "  # dan mengubah ukurannya menjadi 224x224, dan lalu pre-proses\r\n",
        "  face = image [startY:endY, startX:endX]\r\n",
        "  face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\r\n",
        "  face = cv2.resize(face, (224, 224))\r\n",
        "  face = img_to_array(face)\r\n",
        "  face = preprocess_input(face)\r\n",
        "  face = np.expand_dims(face, axis=0)\r\n",
        "  \r\n",
        "  # Membaca wajah dengan model\r\n",
        "  (Marah, Sedih, Senang) = model.predict(face)[0]\r\n",
        "  \r\n",
        "  # Marah warna merah, sedih warna hijau dan senang warna biru\r\n",
        "  if (Marah>Sedih and Marah>Senang):\r\n",
        "    label = \"marah\"\r\n",
        "  elif (Sedih>Senang and Sedih>Marah):\r\n",
        "    label = \"sedih\"\r\n",
        "  elif (Senang>Sedih and Senang>Marah):\r\n",
        "    label = \"senang\"\r\n",
        "\r\n",
        "  if label == \"sedih\":\r\n",
        "    color = (0, 255, 0)\r\n",
        "  elif label == \"marah\":\r\n",
        "    color = (0, 0, 255)\r\n",
        "  elif label == \"senang\":\r\n",
        "    color = (255, 0, 0)\r\n",
        "  \r\n",
        "  # Probabilitas hasil deteksi\r\n",
        "  label = \"{}: {:.2f}%\".format(label, max(Marah, Sedih, Senang) * 100)\r\n",
        "  \r\n",
        "  # Menampilkan hasil dengan label dan kotak\r\n",
        "  cv2.putText(image, label, (startX, startY - 10),\r\n",
        "              cv2.FONT_HERSHEY_TRIPLEX, 1, color, 2)\r\n",
        "  cv2.rectangle(image, (startX, startY), (endX, endY), color, 2)\r\n",
        "  \r\n",
        "# Menampilkan output\r\n",
        "cv2_imshow(image)\r\n",
        "cv2.waitKey(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-snNcM52fzmr"
      },
      "source": [
        "# Mengimport lib\r\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\r\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\r\n",
        "from tensorflow.keras.models import load_model\r\n",
        "from imutils.video import VideoStream\r\n",
        "import numpy as np\r\n",
        "import argparse\r\n",
        "import imutils\r\n",
        "import time\r\n",
        "import cv2\r\n",
        "import os\r\n",
        "\r\n",
        "\r\n",
        "def detect_and_predict_emotion(frame, faceNet, emotionNet):\r\n",
        "  #membuat dimensi kotak deteksi\r\n",
        "  (h, w) = frame.shape[:2]\r\n",
        "  blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300),\r\n",
        "                               (104.0, 177.0, 123.0))\r\n",
        " \r\n",
        "  # Melewatkan blob dan mendeteksi wajah\r\n",
        "  faceNet.setInput(blob)\r\n",
        "  detections = faceNet.forward()\r\n",
        " \r\n",
        "  # Inisialisasi\r\n",
        "  faces = []\r\n",
        "  locs = []\r\n",
        "  preds = []\r\n",
        " \r\n",
        "  for i in range(0, detections.shape[2]):\r\n",
        "    # ekstrak keyakinan (yaitu, probabilitas) yang terkait dengan deteksi\r\n",
        "    confidence = detections[0, 0, i, 2]\r\n",
        "    \r\n",
        "    if confidence > 0.5:\r\n",
        "      # Menghitung koordinat (x, y) dari kotak pembatas untuk objek\r\n",
        "      box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\r\n",
        "      (startX, startY, endX, endY) = box.astype(\"int\")\r\n",
        " \r\n",
        "      # Memastikan kotak pembatas berada dalam dimensi bingkai\r\n",
        "      (startX, startY) = (max(0, startX), max(0, startY))\r\n",
        "      (endX, endY) = (min(w - 1, endX), min(h - 1, endY))\r\n",
        " \r\n",
        "      # Ekstrak ROI wajah, konversikan dari BGR ke pemesanan saluran RGB,\r\n",
        "      # dan mengubah ukurannya menjadi 224x224, dan lalu pre-proses\r\n",
        "      face = frame[startY:endY, startX:endX]\r\n",
        "      face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\r\n",
        "      face = cv2.resize(face, (224, 224))\r\n",
        "      face = img_to_array(face)\r\n",
        "      face = preprocess_input(face)\r\n",
        " \r\n",
        "      # Menambahkan kotak deteksi\r\n",
        "      faces.append(face)\r\n",
        "      locs.append((startX, startY, endX, endY))\r\n",
        "      \r\n",
        "    if len(faces) > 0:\r\n",
        "      faces = np.array(faces, dtype=\"float32\")\r\n",
        "      preds = emotionNet.predict(faces, batch_size=128)\r\n",
        "    return (locs, preds)\r\n",
        " \r\n",
        "faceNet=cv2.dnn.readNet('/content/drive/MyDrive/Colab Notebooks/Face_detector/deploy.prototxt','/content/drive/MyDrive/Colab Notebooks/Face_detector/res10_300x300_ssd_iter_140000.caffemodel')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxtlMKUYf7dd"
      },
      "source": [
        "import base64\r\n",
        "import html\r\n",
        "import io\r\n",
        "import time\r\n",
        " \r\n",
        "from IPython.display import display, Javascript\r\n",
        "from google.colab.output import eval_js\r\n",
        "import numpy as np\r\n",
        "from PIL import Image\r\n",
        "import cv2\r\n",
        " \r\n",
        "def start_input():\r\n",
        "  js = Javascript('''\r\n",
        "    var video;\r\n",
        "    var div = null;\r\n",
        "    var stream;\r\n",
        "    var captureCanvas;\r\n",
        "    var imgElement;\r\n",
        "    var labelElement;\r\n",
        "    \r\n",
        "    var pendingResolve = null;\r\n",
        "    var shutdown = false;\r\n",
        "    \r\n",
        "    function removeDom() {\r\n",
        "       stream.getVideoTracks()[0].stop();\r\n",
        "       video.remove();\r\n",
        "       div.remove();\r\n",
        "       video = null;\r\n",
        "       div = null;\r\n",
        "       stream = null;\r\n",
        "       imgElement = null;\r\n",
        "       captureCanvas = null;\r\n",
        "       labelElement = null;\r\n",
        "    }\r\n",
        "    \r\n",
        "    function onAnimationFrame() {\r\n",
        "      if (!shutdown) {\r\n",
        "        window.requestAnimationFrame(onAnimationFrame);\r\n",
        "      }\r\n",
        "      if (pendingResolve) {\r\n",
        "        var result = \"\";\r\n",
        "        if (!shutdown) {\r\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 512, 512);\r\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\r\n",
        "        }\r\n",
        "        var lp = pendingResolve;\r\n",
        "        pendingResolve = null;\r\n",
        "        lp(result);\r\n",
        "      }\r\n",
        "    }\r\n",
        "    \r\n",
        "    async function createDom() {\r\n",
        "      if (div !== null) {\r\n",
        "        return stream;\r\n",
        "      }\r\n",
        "      div = document.createElement('div');\r\n",
        "      div.style.border = '2px solid black';\r\n",
        "      div.style.padding = '3px';\r\n",
        "      div.style.width = '100%';\r\n",
        "      div.style.maxWidth = '600px';\r\n",
        "      document.body.appendChild(div);\r\n",
        "      \r\n",
        "      const modelOut = document.createElement('div');\r\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\r\n",
        "      labelElement = document.createElement('span');\r\n",
        "      labelElement.innerText = 'No data';\r\n",
        "      labelElement.style.fontWeight = 'bold';\r\n",
        "      modelOut.appendChild(labelElement);\r\n",
        "      div.appendChild(modelOut);\r\n",
        "           \r\n",
        "      video = document.createElement('video');\r\n",
        "      video.style.display = 'block';\r\n",
        "      video.width = div.clientWidth - 6;\r\n",
        "      video.setAttribute('playsinline', '');\r\n",
        "      video.onclick = () => { shutdown = true; };\r\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\r\n",
        "          {video: { facingMode: \"environment\"}});\r\n",
        "      div.appendChild(video);\r\n",
        "      imgElement = document.createElement('img');\r\n",
        "      imgElement.style.position = 'absolute';\r\n",
        "      imgElement.style.zIndex = 1;\r\n",
        "      imgElement.onclick = () => { shutdown = true; };\r\n",
        "      div.appendChild(imgElement);\r\n",
        "      \r\n",
        "      const instruction = document.createElement('div');\r\n",
        "      instruction.innerHTML = \r\n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\r\n",
        "          'Ketika selesai, klik disini atau pada video untuk berhenti dari demo</span>';\r\n",
        "      div.appendChild(instruction);\r\n",
        "      instruction.onclick = () => { shutdown = true; };\r\n",
        "      \r\n",
        "      video.srcObject = stream;\r\n",
        "      await video.play();\r\n",
        "      captureCanvas = document.createElement('canvas');\r\n",
        "      captureCanvas.width = 512; //video.videoWidth;\r\n",
        "      captureCanvas.height = 512; //video.videoHeight;\r\n",
        "      window.requestAnimationFrame(onAnimationFrame);\r\n",
        "      \r\n",
        "      return stream;\r\n",
        "    }\r\n",
        "    async function takePhoto(label, imgData) {\r\n",
        "      if (shutdown) {\r\n",
        "        removeDom();\r\n",
        "        shutdown = false;\r\n",
        "        return '';\r\n",
        "      }\r\n",
        "      var preCreate = Date.now();\r\n",
        "      stream = await createDom();\r\n",
        "      \r\n",
        "      var preShow = Date.now();\r\n",
        "      if (label != \"\") {\r\n",
        "        labelElement.innerHTML = label;\r\n",
        "      }\r\n",
        "            \r\n",
        "      if (imgData != \"\") {\r\n",
        "        var videoRect = video.getClientRects()[0];\r\n",
        "        imgElement.style.top = videoRect.top + \"px\";\r\n",
        "        imgElement.style.left = videoRect.left + \"px\";\r\n",
        "        imgElement.style.width = videoRect.width + \"px\";\r\n",
        "        imgElement.style.height = videoRect.height + \"px\";\r\n",
        "        imgElement.src = imgData;\r\n",
        "      }\r\n",
        "      \r\n",
        "      var preCapture = Date.now();\r\n",
        "      var result = await new Promise(function(resolve, reject) {\r\n",
        "        pendingResolve = resolve;\r\n",
        "      });\r\n",
        "      shutdown = false;\r\n",
        "      \r\n",
        "      return {'create': preShow - preCreate, \r\n",
        "              'show': preCapture - preShow, \r\n",
        "              'capture': Date.now() - preCapture,\r\n",
        "              'img': result};\r\n",
        "    }\r\n",
        "    ''')\r\n",
        " \r\n",
        "  display(js)\r\n",
        "  \r\n",
        "def take_photo(label, img_data):\r\n",
        "  data = eval_js('takePhoto(\"{}\", \"{}\")'.format(label, img_data))\r\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJVEUGHpgAZt"
      },
      "source": [
        "def js_reply_to_image(js_reply):\r\n",
        "    \"\"\"\r\n",
        "    input: \r\n",
        "          js_reply: JavaScript object, contain image from webcam\r\n",
        "    output: \r\n",
        "          image_array: image array RGB size 512 x 512 from webcam\r\n",
        "    \"\"\"\r\n",
        "    jpeg_bytes = base64.b64decode(js_reply['img'].split(',')[1])\r\n",
        "    image_PIL = Image.open(io.BytesIO(jpeg_bytes))\r\n",
        "    image_array = np.array(image_PIL)\r\n",
        " \r\n",
        "    return image_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKCAcX4EgDx4"
      },
      "source": [
        "start_input()\r\n",
        "label_html = 'Capturing...'\r\n",
        "img_data = ''\r\n",
        "count = 0 \r\n",
        "from google.colab.patches import cv2_imshow\r\n",
        "while True:\r\n",
        "  js_reply = take_photo(label_html, img_data)\r\n",
        "  if not js_reply:\r\n",
        "    break\r\n",
        "  \r\n",
        "  image = js_reply_to_image(js_reply)\r\n",
        "  \r\n",
        "  # Mengambil frame dari aliran video berulir dan \r\n",
        "  # ukurannya maksimum lebar 400 pixel\r\n",
        "  frame = image\r\n",
        "  v=True\r\n",
        "  if v == True:\r\n",
        "    \r\n",
        "    frame = imutils.resize(frame, width=400)\r\n",
        "    \r\n",
        "    # Mendeteksi emosi\r\n",
        "    (locs, preds) = detect_and_predict_emotion(frame, faceNet, model)\r\n",
        "    for (box, pred) in zip(locs, preds):\r\n",
        "      \r\n",
        "      # Membuka kotak dan prediksi\r\n",
        "      (startX, startY, endX, endY) = box\r\n",
        "      (Marah, Sedih, Senang) = pred\r\n",
        "\r\n",
        "      # Menggunakan masker hijau, tidak bermasker merah\r\n",
        "      if (Marah>Sedih and Marah>Senang):\r\n",
        "        label = \"sedih\"\r\n",
        "      elif (Sedih>Senang and Sedih>Marah):\r\n",
        "        label = \"marah\"\r\n",
        "      elif (Senang>Marah and Senang>Sedih):\r\n",
        "        label = \"senang\"\r\n",
        "        \r\n",
        "      if label == \"marah\":\r\n",
        "        color = (0, 255, 0)\r\n",
        "      elif label == \"sedih\":\r\n",
        "        color = (0, 0, 255)\r\n",
        "      elif label == \"senang\" :\r\n",
        "        color = (255, 0, 0)\r\n",
        "        \r\n",
        "      # Probabilitas pada label\r\n",
        "      label = \"{}: {:.2f}%\".format(label, max(Marah, Sedih, Senang) * 100)\r\n",
        "      \r\n",
        "      # Menampilkan hasil dengan label dan kotak dari frame\r\n",
        "      frame=cv2.putText(frame, label, (startX, startY - 10),cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\r\n",
        "      frame=cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\r\n",
        "      # Menampilkan ouput\r\n",
        "      cv2_imshow(frame)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}